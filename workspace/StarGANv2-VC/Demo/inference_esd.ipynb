{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cb92b0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/StarGANv2-VC\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a2303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2531e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import Munch\n",
    "from parallel_wavegan.utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5bbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.JDC.model import JDCNet\n",
    "from models import Generator, MappingNetwork, StyleEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54aac22b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Audio / mel utilities\n",
    "# -----------------------\n",
    "def build_mel_transform(n_mels=80, n_fft=2048, win_length=1200, hop_length=300):\n",
    "    return torchaudio.transforms.MelSpectrogram(\n",
    "        n_mels=n_mels, n_fft=n_fft, win_length=win_length, hop_length=hop_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98298063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wave_to_mel(wave, to_mel, mean=-4.0, std=4.0):\n",
    "    wave_t = torch.from_numpy(wave).float()\n",
    "    mel = to_mel(wave_t)  # [n_mels, T]\n",
    "    mel = (torch.log(1e-5 + mel.unsqueeze(0)) - mean) / std  # [1, n_mels, T]\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600e6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Model builders\n",
    "# -----------------------\n",
    "def build_starganv2(model_params):\n",
    "    args = Munch(model_params)\n",
    "    generator = Generator(\n",
    "        args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel\n",
    "    )\n",
    "    mapping_network = MappingNetwork(\n",
    "        args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim\n",
    "    )\n",
    "    style_encoder = StyleEncoder(\n",
    "        args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim\n",
    "    )\n",
    "    nets = Munch(generator=generator, mapping_network=mapping_network, style_encoder=style_encoder)\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16069de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Style helpers\n",
    "# -----------------------\n",
    "def compute_style_from_pairs(style_pairs, starganv2, device, sr, to_mel):\n",
    "    \"\"\"\n",
    "    style_pairs: dict[key] = (ref_path_or_empty, domain_int)\n",
    "      - if path == \"\", style is sampled from mapping_network latent\n",
    "      - else path -> style_encoder(ref_mel, label)\n",
    "    returns: dict[key] = (style_tensor, label_tensor)\n",
    "    \"\"\"\n",
    "    ref_embeds = {}\n",
    "    for key, (p, dom) in style_pairs.items():\n",
    "        label = torch.LongTensor([dom]).to(device)\n",
    "        if not p:\n",
    "            latent_dim = starganv2.mapping_network.shared[0].in_features\n",
    "            with torch.no_grad():\n",
    "                style = starganv2.mapping_network(torch.randn(1, latent_dim).to(device), label)\n",
    "        else:\n",
    "            wave, wav_sr = librosa.load(p, sr=None, mono=True)\n",
    "            if wav_sr != sr:\n",
    "                wave = librosa.resample(wave, orig_sr=wav_sr, target_sr=sr)\n",
    "            wave, _ = librosa.effects.trim(wave, top_db=30)\n",
    "            mel = preprocess_wave_to_mel(wave, to_mel).to(device)\n",
    "            with torch.no_grad():\n",
    "                style = starganv2.style_encoder(mel.unsqueeze(1), label)  # [1, style_dim]\n",
    "        ref_embeds[key] = (style, label)\n",
    "    return ref_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149d661f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ESD speaker mapping\n",
    "# -----------------------\n",
    "def esd_spk_to_domain(spk_id_str):\n",
    "    # 0011..0020 -> 0..9\n",
    "    spk = str(spk_id_str).zfill(4)\n",
    "    if not (spk.isdigit() and 11 <= int(spk) <= 20):\n",
    "        raise ValueError(\"target_speaker must be 0011..0020\")\n",
    "    return int(spk) - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caff39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_wav_from_esd_root(esd_root, spk):\n",
    "    paths = glob.glob(os.path.join(esd_root, spk, \"**\", \"*.wav\"), recursive=True)\n",
    "    paths = [p for p in paths if os.path.isfile(p)]\n",
    "    random.shuffle(paths)\n",
    "    return paths[0] if paths else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd16f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Main inference\n",
    "# -----------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"StarGANv2-VC ESD inference (24kHz)\")\n",
    "    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to StarGANv2-VC config.yml\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, required=True, help=\"Path to model checkpoint .pth\")\n",
    "    parser.add_argument(\"--source\", type=str, required=True, help=\"Path to source wav\")\n",
    "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output directory\")\n",
    "    parser.add_argument(\"--ref\", type=str, default=\"\", help=\"Path to reference wav (style-encoder mode). If empty, use mapping mode.\")\n",
    "    parser.add_argument(\"--target_speaker\", type=str, default=\"0015\", help=\"ESD speaker id 0011..0020\")\n",
    "    parser.add_argument(\"--esd_root\", type=str, default=\"\", help=\"Optional ESD root to auto-pick a reference file when --ref is empty\")\n",
    "    parser.add_argument(\"--vocoder\", type=str, default=\"Vocoder/checkpoint-400000steps.pkl\", help=\"Parallel WaveGAN vocoder checkpoint\")\n",
    "    parser.add_argument(\"--sr\", type=int, default=24000, help=\"Sampling rate for inference\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"cuda or cpu\")\n",
    "    parser.add_argument(\"--save_name\", type=str, default=\"converted.wav\", help=\"Output wav filename\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    device = torch.device(args.device if torch.cuda.is_available() and args.device == \"cuda\" else \"cpu\")\n",
    "\n",
    "    # Load config\n",
    "    with open(args.config, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Build mel transform from config (must match training)\n",
    "    sp = cfg.get(\"preprocess_params\", {}).get(\"spect_params\", {})\n",
    "    to_mel = build_mel_transform(\n",
    "        n_mels=80 if sp.get(\"n_mels\") is None else sp.get(\"n_mels\"),\n",
    "        n_fft=sp.get(\"n_fft\", 2048),\n",
    "        win_length=sp.get(\"win_length\", 1200),\n",
    "        hop_length=sp.get(\"hop_length\", 300),\n",
    "    )\n",
    "\n",
    "    # Build F0 model\n",
    "    f0_model = JDCNet(num_class=1, seq_len=192)\n",
    "    f0_params = torch.load(\"Utils/JDC/bst.t7\")[\"net\"]\n",
    "    f0_model.load_state_dict(f0_params)\n",
    "    f0_model = f0_model.eval().to(device)\n",
    "\n",
    "    # Build vocoder\n",
    "    vocoder = load_model(args.vocoder).to(device).eval()\n",
    "    vocoder.remove_weight_norm()\n",
    "    _ = vocoder.eval()\n",
    "\n",
    "    # Build StarGANv2-VC\n",
    "    nets = build_starganv2(cfg[\"model_params\"])\n",
    "    ckpt = torch.load(args.checkpoint, map_location=\"cpu\")\n",
    "    weights = ckpt.get(\"model_ema\", ckpt.get(\"model\", ckpt))\n",
    "    for k in nets:\n",
    "        nets[k].load_state_dict(weights[k])\n",
    "        nets[k] = nets[k].eval().to(device)\n",
    "\n",
    "    # Load source wav\n",
    "    src_wave, src_sr = librosa.load(args.source, sr=None, mono=True)\n",
    "    if src_sr != args.sr:\n",
    "        src_wave = librosa.resample(src_wave, orig_sr=src_sr, target_sr=args.sr)\n",
    "    src_wave = src_wave.astype(np.float32)\n",
    "    if np.max(np.abs(src_wave)) > 0:\n",
    "        src_wave = src_wave / np.max(np.abs(src_wave))\n",
    "\n",
    "    source_mel = preprocess_wave_to_mel(src_wave, to_mel).to(device)\n",
    "\n",
    "    # Decide target domain and style\n",
    "    dom = esd_spk_to_domain(args.target_speaker)\n",
    "\n",
    "    # If ref not given and esd_root provided, auto-pick a ref file from that speaker\n",
    "    ref_path = args.ref\n",
    "    if not ref_path and args.esd_root:\n",
    "        candidate = any_wav_from_esd_root(args.esd_root, args.target_speaker)\n",
    "        if candidate:\n",
    "            ref_path = candidate\n",
    "\n",
    "    # Prepare style\n",
    "    style_pairs = {\"target\": (ref_path, dom)}  # \"\" -> mapping mode, non-empty -> style encoder mode\n",
    "    ref_embeds = compute_style_from_pairs(style_pairs, nets, device, args.sr, to_mel)\n",
    "    style, label = ref_embeds[\"target\"]\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        f0_feat = f0_model.get_feature_GAN(source_mel.unsqueeze(1))\n",
    "        out_mel = nets.generator(source_mel.unsqueeze(1), style, F0=f0_feat)  # [1, 80, T]\n",
    "        c = out_mel.transpose(-1, -2).squeeze().to(device)                    # [T, 80]\n",
    "        y_out = vocoder.inference(c).view(-1).cpu().numpy()\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(args.output, args.save_name)\n",
    "    sf.write(out_path, y_out, args.sr, subtype=\"PCM_16\")\n",
    "    print(f\"[OK] Saved: {out_path}\")\n",
    "\n",
    "    # Optional: also save trimmed source and ref copies for audit\n",
    "    try:\n",
    "        sf.write(os.path.join(args.output, \"_src_24k.wav\"), src_wave, args.sr, subtype=\"PCM_16\")\n",
    "        if ref_path:\n",
    "            ref_wav, ref_sr = librosa.load(ref_path, sr=None, mono=True)\n",
    "            if ref_sr != args.sr:\n",
    "                ref_wav = librosa.resample(ref_wav, orig_sr=ref_sr, target_sr=args.sr)\n",
    "            sf.write(os.path.join(args.output, \"_ref_24k.wav\"), ref_wav, args.sr, subtype=\"PCM_16\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Audit save failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4b8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --config CONFIG --checkpoint CHECKPOINT\n",
      "                             --source SOURCE --output OUTPUT [--ref REF]\n",
      "                             [--target_speaker TARGET_SPEAKER]\n",
      "                             [--esd_root ESD_ROOT] [--vocoder VOCODER]\n",
      "                             [--sr SR] [--device DEVICE]\n",
      "                             [--save_name SAVE_NAME] [--seed SEED]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --config, --checkpoint, --source, --output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647825db-7b2a-4a6f-b893-7fd17be5b0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
